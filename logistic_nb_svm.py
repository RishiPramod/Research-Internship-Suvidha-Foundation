# -*- coding: utf-8 -*-
"""Logistic_NB_SVM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HjJ_slOEpeMBJe2fZkrU9FTf7Z_tKkCJ
"""

!pip install rouge

import pandas as pd
import numpy as np
import re
import string


from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


from rouge import Rouge


import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# =============================
# 1. Load and Preprocess Dataset
# =============================
df = pd.read_csv("/content/drive/MyDrive/The_Hindu_paper_dataset_2015_to_2025.csv")

# Basic cleaning
def clean_text(text):
    text = re.sub(r"\d{1,2}-\d{1,2}-\d{4}", "", str(text)) # remove dates
    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    return text.strip()


df["Content_clean"] = df["Content"].apply(clean_text)
df["Headline_clean"] = df["Headline"].apply(clean_text)


X = df["Content_clean"]
y = df["Headline_clean"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# =============================
# 2. Classical Models
# =============================
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)


models = {
    "LogisticRegression": LogisticRegression(max_iter=1000),
    "NaiveBayes": MultinomialNB(),
    "SVM": LinearSVC()
}


results = {}


for name, model in models.items():
    model.fit(X_train_tfidf, y_train)
    preds = model.predict(X_test_tfidf)


    acc = accuracy_score(y_test, preds)
    prec = precision_score(y_test, preds, average="macro", zero_division=0)
    rec = recall_score(y_test, preds, average="macro", zero_division=0)
    f1 = f1_score(y_test, preds, average="macro", zero_division=0)


    rouge = Rouge()
    rouge_scores = rouge.get_scores(preds.tolist(), y_test.tolist(), avg=True)


    results[name] = {
        "Accuracy": acc,
        "Precision": prec,
        "Recall": rec,
        "F1": f1,
        "ROUGE-1": rouge_scores["rouge-1"]["f"],
        "ROUGE-2": rouge_scores["rouge-2"]["f"],
        "ROUGE-L": rouge_scores["rouge-l"]["f"]
    }



"""## 3. Deep Learning Model"""

# =============================
# 3. Deep Learning Models (LSTM, CNN, PHT)
# =============================
# Vocabulary
all_text = " ".join(X_train)
vocab = {word: i+1 for i, word in enumerate(set(all_text.split()))}
vocab_size = len(vocab) + 1


class NewsDataset(Dataset):
    def __init__(self, texts, labels, vocab, max_len=100):
        self.texts = [[vocab.get(word, 0) for word in text.split()[:max_len]] for text in texts]
        self.labels = labels
        self.max_len = max_len


    def __len__(self):
        return len(self.texts)


    def __getitem__(self, idx):
        x = self.texts[idx]
        x = x + [0]*(self.max_len - len(x))
        return torch.tensor(x, dtype=torch.long), self.labels[idx]


# Simple Encoder Models
class LSTMModel(nn.Module):
    def __init__(self, vocab_size, embed_dim=128, hidden_dim=128, output_dim=128):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
    def forward(self, x):
        emb = self.embedding(x)
        _, (h, _) = self.lstm(emb)
        return self.fc(h[-1])


class CNNModel(nn.Module):
    def __init__(self, vocab_size, embed_dim=128, num_filters=100, filter_sizes=[3,4,5], output_dim=128):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.convs = nn.ModuleList([nn.Conv2d(1, num_filters, (fs, embed_dim)) for fs in filter_sizes])
        self.fc = nn.Linear(len(filter_sizes)*num_filters, output_dim)
    def forward(self, x):
        emb = self.embedding(x).unsqueeze(1)
        conved = [torch.relu(conv(emb)).squeeze(3) for conv in self.convs]
        pooled = [torch.max(c, dim=2)[0] for c in conved]
        cat = torch.cat(pooled, dim=1)
        return self.fc(cat)


class PHTModel(nn.Module):
    def __init__(self, vocab_size, embed_dim=128, nhead=4, num_layers=2, output_dim=128):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=nhead)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.fc = nn.Linear(embed_dim, output_dim)
    def forward(self, x):
        emb = self.embedding(x)
        out = self.transformer(emb)
        return self.fc(out.mean(dim=1))

# =============================
# Training Loop
# =============================
def train_model(model, train_loader, test_loader, epochs=3, lr=1e-3):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)


    for epoch in range(epochs):
        model.train()
        for x, y in train_loader:
            x = x.to(device)
            # Simplified: using text matching classification surrogate (needs label encoding)
            optimizer.zero_grad()
            preds = model(x)
            loss = criterion(preds, torch.randint(0, preds.shape[1], (x.shape[0],), device=device))
            loss.backward()
            optimizer.step()

    return model

# =============================
# 4. TODO: Encode labels, run training, evaluate deep models with same metrics
# =============================


import pprint
pprint.pprint(results)

